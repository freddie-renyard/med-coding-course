{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING\n",
    "\n",
    "## Thought Processes\n",
    "- Missing data is dealt with, cleaning data is dealt with, standard scaler is dealt with. \n",
    "- But first, we should do some housekeeping\n",
    "\n",
    "- How should we model this? \n",
    "- single variable 'Y' with a binary outcome so multivariable analysis for binary outcome\n",
    "- approaches: flexible non parametric approaches vs. parametric approaches\n",
    "- recommendation: Linear Model vs. Tree based model\n",
    "- Multivariable logistic regression and within this we will try penalised regression. vs. \n",
    "- Tree based model [bagged vs boosted model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we assess model fitting?\n",
    "- Train / Test split\n",
    "When do we do train / test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs. Variance Tradeoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What libraries will we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need to do before we start modelling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Logistic Regression and Penalisation \n",
    "- How does it work?\n",
    "- what are its flaws?\n",
    "\n",
    "### Tree based models\n",
    "- how does it work?\n",
    "- what are its flaws?\n",
    "\n",
    "### Naive Bayes \n",
    "\n",
    "### Support Vector Machines\n",
    "- how does it work \n",
    "\n",
    "How do you find out the flaws? \n",
    "- assumptions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering \n",
    "\n",
    "- GLMs (Generalised Linear Models) usually do not require feature engineering significantly. \n",
    "- GLMs assume 'normality' of features and most importantly, 'normality of errors'. \n",
    "- This assumption makes it 'parametric' . Parametric means it fulfils the pre-defined 'parameters'. \n",
    "- Ordinary Least Squares (OLS) Linear regression's link function is itself. i.e., The coefficients and output is not constraint to target space.\n",
    "- The good thing is you can just use your X variables as it is.\n",
    "- And, because of assumption of linearity, you can interpret X variables beyond the feature space. \n",
    "\n",
    "- Logistic Regression is where output variables is transformed into binary value of 0 or 1. I.e., constraint to feature space of 1. \n",
    "- This is achieved by a link function which transfrorms the output into 0 or 1.\n",
    "- Therefore you cannot interpret outputs in the same as linear regression. \n",
    "- Outputs are interpreted as 'odds ratio'. \n",
    "- But, because of linearity assumption, you can interpret X variables beyond the feature space.\n",
    "\n",
    "- Machine Learning models (not all) and more complex linear regression models such as Penalised regression required variables to be on a same feature space.\n",
    "- This is because ML models work based on some sort of 'distance measures' \n",
    "- Support Vector Machines model which aims to find a hyper-plane works using some sort of distance measures.\n",
    "- Distance Measures e.g., Euclidean distance uses Pythagorean theorem.\n",
    "- To illustrate, say height is a variable measured in metres with a range of 120-200 cm.\n",
    "- Then, procalcitonin is measured in ranges 0.01 to 0.1 \n",
    "- Then, salary is measured in 10,000 to 100,000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How do we evaluate the model?\n",
    "- What does success look like?\n",
    "- We are predicting for 'null' \n",
    "- So we want a specific model not 'sensitive'. \n",
    "- edge cases\n",
    "- where the model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import statsmodels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
